{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ae3d36",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Session 2 Part 2: Using the sliding window technique to predict on larger images\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Florient Chouteau | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>\n",
    "\n",
    "In this session we will load our previously detected model and apply it on large images using the sliding window technique.\n",
    "\n",
    "The sliding window technique is a method to convert a classifier into detector. It can be illustrated by a single gif:\n",
    "\n",
    "![sw](https://storage.googleapis.com/fchouteau-isae-deep-learning/static/sliding_window.gif)\n",
    "\n",
    "For more information about the sliding window technique refer to this excellent article:\n",
    "\n",
    "https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/\n",
    "\n",
    "*Note:* We are training our model to recognize images at a single scale. Satellite imagery more or less prevents the foreground/background effect that sometimes require large changes in scale between training and testing for \"normal\" photography. So you can ignore the bits about the image pyramid on this issue (it is very good for general culture though, and can be applied in other use cases, or if we used multiscale training to \"zoom\" small aircrafts for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your imports here\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4601099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "tiles_dataset_url = \"https://storage.googleapis.com/fchouteau-isae-deep-learning/tiles_aircraft_dataset.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8579a",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "ds = np.DataSource(\"/tmp/\")\n",
    "f = ds.open(tiles_dataset_url, \"rb\")\n",
    "eval_tiles = np.load(f)\n",
    "eval_tiles = eval_tiles[\"eval_tiles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c3f19",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "- Plot some of the images\n",
    "- The images are not labelled to prevent any \"competition\", the objective is just to apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 2\n",
    "grid = np.zeros((grid_size * 512, grid_size * 512, 3)).astype(np.uint8)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        tile = np.copy(eval_tiles[np.random.randint(0, eval_tiles.shape[0])])\n",
    "        grid[i * 512 : (i + 1) * 512, j * 512 : (j + 1) * 512, :] = tile\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac1dfb",
   "metadata": {},
   "source": [
    "## Reload your model\n",
    "\n",
    "- Using the routines detailed in the previous notebook, upload the scripted model corresponding to the best training (don't forget to save it on the other notebooks) then reload the model\n",
    "\n",
    "- Find the mean / std of the dataset you trained with to normalize the images !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a47ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for fn in uploaded.keys():\n",
    "#     print(\n",
    "#         'User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#             name=fn, length=len(uploaded[fn])\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.jit\n",
    "\n",
    "MODEL = torch.jit.load(\"scripted_model.pt\", map_location=\"cpu\")\n",
    "MODEL = MODEL.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = ...\n",
    "\n",
    "STD = ...\n",
    "\n",
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(MEAN, STD),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45eb720",
   "metadata": {},
   "source": [
    "## Implement the sliding window\n",
    "\n",
    "Intuitively, it's about applying an aircraft classifier trained on 64x64 pictures of aircraft or \"anything else\" as a detector.\n",
    "\n",
    "Our network structure more or less prevents applying it to the full 512x512 images, and even if it could (you may be able to do it with global pooling layers...) this would not bring much information (\"there is at least one aircraft in this region\" sometimes is not sufficient).\n",
    "\n",
    "So the idea is to \"slide\" our 64x64 classifier on the image and collect the coordinates where \"aircraft\" is returned. Those should be the locations of our aircrafts;\n",
    "\n",
    "You could view your model as a big convolution returning \"aircraft / not aircraft\". Its kernel size is 64x64, there are one or two filters depending on if you coded with softmax or crossentropy. You then just have to decide on the stride of this convolution... And to keep in mind how to go back to coordinates to plot your aircrafts afterwards ;)\n",
    "\n",
    "There are a lot of degrees of freedom when developping sliding windows. A sliding window with a too small \"step\" will only provide noisy overlapping detections. A step too large will make you miss some objects.\n",
    "\n",
    "It's up to you to find acceptable parameters.\n",
    "\n",
    "*Note*: The dataset labels were generated so that an image is considered an aircraft **if and only if the center of an aircraft lies in the center 32x32** of the 64x64 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_on_large_image(\n",
    "    img: np.ndarray, model: nn.Module, patch_size=64, patch_stride=32\n",
    "):\n",
    "    h, w, c = img.shape\n",
    "    coords = []\n",
    "\n",
    "    for i0 in range(0, h - patch_size + 1, patch_stride):\n",
    "        for j0 in range(0, w - patch_size + 1, patch_stride):\n",
    "            patch = img[i0 : i0 + patch_size, j0 : j0 + patch_size]\n",
    "            patch = image_transforms(patch).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(patch)\n",
    "                y_pred = y_pred[0, 0].cpu().numpy()\n",
    "                if y_pred > 0.5:\n",
    "                    coords.append((i0 + 32, j0 + 32))\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3b43d",
   "metadata": {},
   "source": [
    "## Apply the sliding window on the dataset and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.random.randint(eval_tiles.shape[0])\n",
    "image = np.copy(eval_tiles[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ff39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = apply_model_on_large_image(image, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_on_image(image: np.ndarray, results: results):\n",
    "    color = (0, 255, 0)\n",
    "\n",
    "    image0 = np.copy(image)\n",
    "\n",
    "    for ic, jc in results:\n",
    "        image = cv2.rectangle(\n",
    "            image, (jc - 32, ic - 32), (jc + 32, ic + 32), color, thickness=2\n",
    "        )\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "    ax[0].imshow(image0)\n",
    "    ax[1].imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52112535",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_on_image(image, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20989a",
   "metadata": {},
   "source": [
    "## What's next ?\n",
    "\n",
    "Well...\n",
    "\n",
    "Are you satisfied with the behaviour of your model ?  Are there a lot of false positives ?\n",
    "\n",
    "If so, you can go back to the previous notebooks to tune your model and re-apply it.\n",
    "\n",
    "If you're out of your depth on how to improve your model... think about it ;)  You should be able to find news ideas because really, those problems have no end\n",
    "\n",
    "Welcome to the life of a DL engineer !"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
