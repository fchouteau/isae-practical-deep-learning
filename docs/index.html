<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>ISAE Practical Deep Learning Session</title>
    <link rel="shortcut icon" href="./favicon.ico" />
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="./static/css/reset.css" />
    <link rel="stylesheet" href="./static/css/evo.css" />
    <!-- <link rel="stylesheet" href="/_assets/evo" id="theme" /> -->
    <link rel="stylesheet" href="./css/highlight/vs.css" />


</head>

<body>
    <div class="reveal">
        <div class="slides"><section ><section data-markdown><script type="text/template">

# Deep Learning in Practice

**ISAE-SUPAERO, SDD, 23 Nov 2022**

Florient CHOUTEAU, Quentin LETURGIE

</script></section><section data-markdown><script type="text/template">

Slides : https://fchouteau.github.io/isae-practical-deep-learning/

Notebooks : https://github.com/SupaeroDataScience/deep-learning/tree/main/vision

</script></section></section><section ><section data-markdown><script type="text/template">

## Detect Aircrafts on Satellite Imagery

![ac](static/img/aircrafts.gif)

</script></section><section data-markdown><script type="text/template">

6 hours hands on session on applying "deep learning" to a "real" use-case

![dog_cat_meme](static/img/dog_meme.jpg) <!-- .element: height="40%" width="40%" -->

</script></section><section data-markdown><script type="text/template">
<!-- .slide: data-background="http://i.giphy.com/90F8aUepslB84.gif" -->

### RevealJS <!-- .element: style="color: white; font-family: serif; font-size: 1.2em;" -->

These slides are built using <!-- .element: style="color: white; font-family: cursive; font-size: 1.2em;" --> [reveal.js](https://revealjs.com) and [reveal-md](
https://github.com/webpro/reveal-md)

This is awesome ! ðŸ˜² <!-- .element: style="color: white; font-family: cursive; font-size: 1.2em;" -->

</script></section><section data-markdown><script type="text/template">

### Who ?

- ![ads](static/img/AIRBUS_Blue.png) <!-- .element: height="44px" width="220px" -->
- SDD 2016
- Computer Vision R&D at **Airbus Defence and Space**
- Ground segment software for Earth Observation satellites
- Daily job revolving around Machine Learning + Satellite Imagery
    - Information extraction
    - Image processing
    - Research stuff

(contact me on slack)

</script></section><section data-markdown><script type="text/template">

### Who ?

You'll see me a bit this year :
- CNN for Computer Vision
- Outils du Big Data : Cloud, Docker, Deployment

</script></section><section data-markdown><script type="text/template">

### Context: Earth Observation

![context](static/img/context.png)  <!-- .element:  width="60%" height="60%"-->

</script></section><section data-markdown><script type="text/template">

### Context: DL on Satellite Imagery

A lot of use cases :

- Land Use / Land Cover cartography
- Urban Cartography (building, roads, damage assessment...)
- Various objects detections (ships, vehicles...)

![shipdet](static/img/airbus_ship_det.png) <!-- .element:  width="60%" height="60%"-->

</script></section><section data-markdown><script type="text/template">

### Context: DL on Satellite Imagery

Can also be used for "image processing" : 

- Denoising
- Super Resolution

![](static/img/pneohd.gif) <!-- .element:  width="20%" height="20%"-->

</script></section><section data-markdown><script type="text/template">

### Context: Needles in haystacks

![pyramid](static/img/large_pyramid.jpg)  <!-- .element:  width="40%" height="40%"-->

</script></section><section data-markdown><script type="text/template">

### What you did last time 

- Trained an ANN & a Convolutional Neural Network on Fashion MNIST
- Wrote your first training loops with Pytorch
- Maybe discovered "callbacks" (early stopping), optimizers (sgd, adam), dropout
- Maybe saw your firsts neural architectures (alexnet, vggs, resnets)
- Maybe discovered pytorch ignite

</script></section><section data-markdown><script type="text/template">

### What we are going to do

Train an aircraft detector on a dataset of aircrafts and "not aircrafts"

- using convolutional neural networks <!-- .element: class="fragment" data-fragment-index="1" -->
- using pytorch <!-- .element: class="fragment" data-fragment-index="2" -->
- using google colaboratory and its GPUs <!-- .element: class="fragment" data-fragment-index="3" -->

![colab](static/img/colab.png) <!-- .element:  class="fragment" data-fragment-index="4" width="25%" height="25%"-->

</script></section><section data-markdown><script type="text/template">

This is a "hands-on", not a full class

**More resources on DL for Computer Vision**

- [http://cs231n.stanford.edu](http://cs231n.stanford.edu/schedule.html)
- [https://d2l.ai/index.html](https://d2l.ai/index.html)


</script></section></section><section ><section data-markdown><script type="text/template">

## Session 1: Hands-On

</script></section><section data-markdown><script type="text/template">

### Objectives

- Launch notebooks on Colab
- Build an intuition over convolutions and CNNs
- Train a basic CNN on a small training set
- Plot the metrics & ROC curve on a small test set
- Discover the world of hyperparameter tuning

</script></section><section data-markdown><script type="text/template">

### Outcomes

- Use GCP to get access to computing power & GPUs
- Handle a dataset of images, do some basic data exploration
- Train & evaluate your first CNN on a simple dataset
- Go beyound accuracy to diagnose your model

</script></section><section data-markdown><script type="text/template">

### Dataset description

- 2600 train images (1300 aircrafts, 1300 background), size 64x64
- 880 test images (440 aircrafts, 440 background), size 64x64

![toy_dataset](static/img/toy_dataset.png) <!-- .element height="40%" width="40%" -->

</script></section><section data-markdown><script type="text/template"> 

### Pytorch reminder

![pt](static/img/pytorch_1.jpeg)  <!-- .element height="50%" width="50%" -->

</script></section><section data-markdown><script type="text/template">

### Pytorch reminder

![pt](static/img/pytorch_2.jpeg)  <!-- .element height="50%" width="50%" -->


</script></section><section data-markdown><script type="text/template">

### Let's go ! 

1. Go to google colab
2. Import the first notebook & follow the guidelines
3. ...
4. Profit !
5. If you're done... go to the next notebook !

</script></section><section data-markdown><script type="text/template">

### Colab Guide

<video data-autoplay  controls width="720">
    <source src="https://storage.googleapis.com/fchouteau-isae-deep-learning/static/colab_guide_proper.mp4" type="video/mp4">
</video>

</script></section><section data-markdown><script type="text/template">

### GPU ???

You'll see that... in February

[Tutorial](http://d2l.ai/chapter_appendix-tools-for-deep-learning/colab.html)

</script></section></section><section ><section data-markdown><script type="text/template">

## Session 1
## Take-Away messages

</script></section><section data-markdown><script type="text/template">

### Kernel filtering

![kernel_filtering](static/img/kernel_filtering.gif) <!-- .element height="30%" width="30%" -->

</script></section><section data-markdown><script type="text/template">

### ConvNets intuition comes from image processing

![convolution_filtering](static/img/convolution_filtering.png) <!-- .element height="60%" width="60%" -->

(I apologize for using Lena as an example)

</script></section><section data-markdown><script type="text/template">

### ConvNets intuition comes from image processing

![convolution](static/img/convolution.gif) <!-- .element height="60%" width="40%" -->

</script></section><section data-markdown><script type="text/template">

**ConvNets works because we assume inputs are images**

![permuted](static/img/permuted.png) <!-- .element height="60%" width="60%" -->

</script></section><section data-markdown><script type="text/template">

### ConvNets

![feature_extraction](static/img/feature_extraction.png)  <!-- .element height="60%" width="60%" -->

</script></section><section data-markdown><script type="text/template">

### Convolutions ?

![cnns](static/img/convolution_slinding.gif)  <!-- .element height="40%" width="40%" -->

[useful link](https://github.com/vdumoulin/conv_arithmetic)

</script></section><section data-markdown><script type="text/template">

### Pooling ?

![mp](static/img/maxpool_animation.gif) <!-- .element height="40%" width="40%" -->

</script></section><section data-markdown><script type="text/template">

### nn.Linear ?

![nn](static/img/nnlinear.png) <!-- .element height="40%" width="40%" -->

</script></section><section data-markdown><script type="text/template">

### Computing shape

![tiles](static/img/cnnshape.png) <!-- .element height="35%" width="35%" -->

</script></section><section data-markdown><script type="text/template">

### CNNs in practice...

![tiles](static/img/torchvision.png) <!-- .element height="35%" width="35%" -->

```text
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
```

</script></section><section data-markdown><script type="text/template">

### ROC-Curves

(see section "extra" on these slides)

</script></section></section><section ><section data-markdown><script type="text/template">

## Session 2
## Class Imbalance & Sliding Windows

</script></section><section data-markdown><script type="text/template">

### Objectives

- Train a CNN on a larger & imbalanced dataset
- Evaluate the performance of a model on imbalanced data
- Try and improve performance
- Apply your model on larger images to detect aircrafts

</script></section><section data-markdown><script type="text/template">

### Trainval Dataset description

- 46000 64x64 train images
- 10240 64x64 test images
- **1/10 aircraft-background ratio**

![tiles](static/img/large_dataset.png) <!-- .element height="35%" width="35%" -->

</script></section><section data-markdown><script type="text/template">

### Final Dataset description

- Objective: Apply your classifier on "real" images and find aircrafts
- 36 512x512 images with some aircrafts

![tiles](static/img/large_tiles.png) <!-- .element height="35%" width="35%" -->

</script></section><section data-markdown><script type="text/template">

### One idea: Sliding window

- Training Image Size: 64x64, output = binary classification
- Target Image Size: 512x512, target = detect & count aircrafts ?

![sliding](static/img/sliding_window.gif)

</script></section><section data-markdown><script type="text/template">

### Outcomes

- Tackle a dataset with huge class imbalance
- Discover more advanced techniques for training CNNs
- Discover Precision-Recall Curves
- Discover applying models on larger images using the sliding window technique

</script></section><section data-markdown><script type="text/template">

### Steps by steps

1. Start/Restart your machine
2. Follow notebooks 2 and 3

![xkcd](https://i.stack.imgur.com/U9Iki.png)

</script></section></section><section ><section data-markdown><script type="text/template">

## Session 2: Take-home messages

</script></section><section data-markdown><script type="text/template">

### Objectives

- Continue manipulating CNNs using pytorch
- Tackle a more realistic dataset
- Examine what must changes to diagnose your model and improve it

</script></section><section data-markdown><script type="text/template">

### At home

- Continue the notebooks
- Ask question on chat
- Look at the Deep Learning classes of [cs231n](http://cs231n.stanford.edu/schedule.html)

</script></section><section data-markdown><script type="text/template">

Welcome to the life of a deep learning engineer !

![train](static/img/model_train_img.png)

</script></section><section data-markdown><script type="text/template">

![data](static/img/tesla.jpg) <!-- .element height="70%" width="70%" -->

</script></section><section data-markdown><script type="text/template">

![goodbye](https://media.giphy.com/media/lD76yTC5zxZPG/giphy.gif)

</script></section></section><section ><section data-markdown><script type="text/template">

## Extra
## Diagnosing Classifier performance

</script></section><section data-markdown><script type="text/template">

### Binary classification metrics

![cm](static/img/confusion_matrix.png)

</script></section><section data-markdown><script type="text/template">

### The ROC Curve

![roc](static/img/roc-curve-v2.png)

</script></section><section data-markdown><script type="text/template">

### The ROC curve (visualized)

![roc](https://raw.githubusercontent.com/dariyasydykova/open_projects/master/ROC_animation/animations/ROC.gif)

The shape of an ROC curve changes when a model changes the way it classifies the two outcomes.

</script></section><section data-markdown><script type="text/template">

### How to compute a ROC curve ?

![proc](https://raw.githubusercontent.com/dariyasydykova/open_projects/master/ROC_animation/animations/cutoff.gif)  <!-- .element height="40%" width="40%" -->

- y_pred = a list of probas, y_true = a list of 0 or 1
- vertical line : threshold value
- red dot : FPR and TPR for the threshold
- the curve is plotted for all available thresholds

</script></section><section data-markdown><script type="text/template">

### Precision & Recall

Usually the most important things in imbalanced classification

![pr](static/img/precision_recall.png)  <!-- .element height="40%" width="40%" -->

</script></section><section data-markdown><script type="text/template">

### PR synthetic metric

![fbeta](static/img/fbeta.png) <!-- .element height="35%" width="35%" -->

- beta = 1 => Recall & Precision weighted equally
- beta > 1 => Emphasizes recall (not missing positive examples)
- beta < 1 => Emphasizes precision (not doing )

</script></section><section data-markdown><script type="text/template">

### The PR Curve

![pr](static/img/pr_curve.png) <!-- .element height="75%" width="75%" -->

</script></section><section data-markdown><script type="text/template">

### The PR Curve (visualized)

![pr](https://raw.githubusercontent.com/dariyasydykova/open_projects/master/ROC_animation/animations/PR.gif)

The shape of the precision-recall curve also changes when a model changes the way it classifies the two outcomes.

</script></section><section data-markdown><script type="text/template">

### Precision-Recall or ROC ?

- Both curve can be used to select your trade-off
- Precision-recall curve is more sensitive to class imbalance than an ROC curve
- Example: Try computing your FPR on very imbalanced dataset

![prroc](https://raw.githubusercontent.com/dariyasydykova/open_projects/master/ROC_animation/animations/imbalance.gif)  <!-- .element height="50%" width="50%" -->

</script></section><section data-markdown><script type="text/template">

### Curves Usage: Selecting trade-off

![calib](static/img/pr_space.png)  <!-- .element height="70%" width="70%" -->

</script></section><section data-markdown><script type="text/template">

Readings:
- https://lukeoakdenrayner.wordpress.com/2018/01/07/the-philosophical-argument-for-using-roc-curves/
- https://towardsdatascience.com/on-roc-and-precision-recall-curves-c23e9b63820c

</script></section></section><section ><section data-markdown><script type="text/template">

## Extra : Pytorch Ecosystem

</script></section><section data-markdown><script type="text/template">

### high-level frameworks over pytorch

- pytorch: define your models, autodifferenciation, **but you write the rest**
- hl library: training loops, callbacks, distribution etc...

![ignite](https://raw.githubusercontent.com/pytorch/ignite/master/assets/ignite_vs_bare_pytorch.png)  <!-- .element height="50%" width="50%" -->

</script></section><section data-markdown><script type="text/template">

### high-level frameworks over pytorch

![lightning](static/img/lightning.jpeg) <!-- .element height="40%" width="40%" -->

</script></section><section data-markdown><script type="text/template">

### ![pytorch-ignite](https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logo_mixed.svg) <!-- .element:  width="15%" height="15%"-->

- [pytorch-ignite](https://github.com/skorch-dev/skorch) : a high-level deep learning library based on top of pytorch
- Reduce boilerplate code (training loops, early stopping, logging...)
- Extensible, based on experiment management

</script></section><section data-markdown><script type="text/template">

### Pytorch Ecosystem 

- There are other high-level frameworks based on pytorch: [Skorch](https://github.com/skorch-dev/skorch), [Lightning](https://github.com/williamFalcon/pytorch-lightning). 
- All of them have their pros and cons
- [There is a huge ecosystem based around pytorch](https://pytorch.org/ecosystem/)

![](static/img/lightning.jpeg) <!-- .element: style="width: 25%; height: 25%"--> 

</script></section></section><section ><section data-markdown><script type="text/template">

### Extra : From classification to Detection

</script></section><section data-markdown><script type="text/template">

We've done image classification that we applied in a sliding window fashion on larger images

![sliding](static/img/sliding_window.gif)

</script></section><section data-markdown><script type="text/template">

We can solve other types of tasks with ConvNets

![tasks](static/img/computervision_tasks.png)  <!-- .element: style="width: 60%; height: 40%"--> 

</script></section><section data-markdown><script type="text/template">

Image Segmentation

![segmentation](https://s3-us-west-2.amazonaws.com/climate-com/images/encoderdecoder2.png)   <!-- .element: style="width: 60%; height: 40%"-->

</script></section><section data-markdown><script type="text/template">

Object Detection (intuition)

![objdet](https://miro.medium.com/max/1400/1*REPHY47zAyzgbNKC6zlvBQ.png)  <!-- .element: style="width: 60%; height: 40%"-->

</script></section><section data-markdown><script type="text/template">

Object Detection (in practice)

![objectdetection](https://www.mdpi.com/applsci/applsci-12-01291/article_deploy/html/images/applsci-12-01291-g001.png)   <!-- .element: style="width: 60%; height: 40%"-->

</script></section><section data-markdown><script type="text/template">

Instance Segmentation

![instseg](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_7.44.34_PM.png)  <!-- .element: style="width: 60%; height: 40%"-->

</script></section><section data-markdown><script type="text/template">

To learn more about this, see [this](http://cs231n.stanford.edu/slides/2022/lecture_9_jiajun.pdf)

</script></section><section data-markdown><script type="text/template">

Other keywords that are important for CNN in Computer Vision

- Self-Supervised Learning
- Vision Transformers
- (Generative Adversarial Networks)
- Diffusion Models (Imagen, StableDiffusion, ...)

</script></section></section></div>
    </div>
    <!-- <div id="footer-container" style="display:none;"> -->
    <div id="footer-container">
        <div id="footer">
            SDD - CNN & Imagery
            <br />
            <a href="https://supaerodatascience.github.io/deep-learning/">https://supaerodatascience.github.io/deep-learning/</a>
            <br />
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License"
                    style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
        </div>
    </div>
    <div id="logo-container">
        <div id="logo">
            <img src="static/img/sdd_logo.png" width="8%">
        </div>
    </div>
    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
        function extend() {
            var target = {};
            for (var i = 0; i < arguments.length; i++) {
                var source = arguments[i];
                for (var key in source) {
                    if (source.hasOwnProperty(key)) {
                        target[key] = source[key];
                    }
                }
            }
            return target;
        }

        // default options to init reveal.js
        var defaultOptions = {
            controls: true,
            progress: true,
            history: true,
            center: true,
            transition: 'default', // none/fade/slide/convex/concave/zoom
            plugins: [
                RevealMarkdown,
                RevealHighlight,
                RevealZoom,
                RevealNotes,
                RevealMath
            ]
        };

        // options from URL query string
        var queryOptions = Reveal().getQueryHash() || {};

        var options = extend(defaultOptions, {"transition":"fade","transitionSpeed":"default","controls":true,"slideNumber":true,"width":"100%","height":"100%"}, queryOptions);
    </script>


    <script>
        Reveal.initialize(options);
        var footer = $('#footer-container').html();
        $('div.reveal').append(footer);
        var logo = $('#logo-container').html();
        $('div.reveal').append(logo);
    </script>
</body>

</html>